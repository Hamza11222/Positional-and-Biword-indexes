{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biword.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXPLUL_IlxAh"
      },
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from natsort import natsorted\n",
        "import string\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4ZhQ-NLTJ-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c420f8e-ff92-42db-ab3c-eb2f7b28128b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5x8uSvPTKCr"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/documents')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72wEjOZHttKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae06cf3e-0d10-40ba-cf5a-e73fec14770e"
      },
      "source": [
        "#  tokenizing to further find positional index\n",
        "\n",
        "import os\n",
        "from _collections import OrderedDict\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "num = 1;\n",
        "for filename in os.listdir(os.getcwd()):\n",
        "    #print(filename)\n",
        "    file7 = open(filename,\"r\")\n",
        "    num = num+1\n",
        "    stemmer = PorterStemmer()\n",
        "    file7 = file7.read();\n",
        "    #print(file7)\n",
        "    word_tokens = word_tokenize(file7)\n",
        "    #print(word_tokens)\n",
        "    filtered_sentence = [w for w in word_tokens]\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        #print(w)\n",
        "        #if w not in stopwords:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "        #print(num)\n",
        "        num=num+1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAqxq3Kkl4BE"
      },
      "source": [
        "\n",
        "# In this example, we create the positional index for  1 folder.\n",
        "folder_names = [\"documents\"]\n",
        " \n",
        "# Initialize the stemmer.\n",
        "stemmer = PorterStemmer()\n",
        " \n",
        "# Initialize the file no.\n",
        "fileno = 0\n",
        " \n",
        "# Initialize the dictionary.\n",
        "bi_index = {}\n",
        " \n",
        "# Initialize the file mapping (fileno -> file name).\n",
        "file_map = {}\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LddnafFgl4Ys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e2eae2-e402-4f91-da92-c0ee548f9dd7"
      },
      "source": [
        "# code for positional index\n",
        "import json\n",
        "def read_file(filename):\n",
        "    with open(filename, 'r', encoding =\"ascii\", errors =\"surrogateescape\") as f:\n",
        "        stuff = f.read()\n",
        " \n",
        "    f.close()\n",
        "     \n",
        "    # Remove header and footer.\n",
        "    \n",
        "     \n",
        "    return stuff\n",
        "\n",
        "def normalize(text):\n",
        "    text = re.sub(\"[^A-Za-z_]\", \" \", text)\n",
        "    text = re.sub(\" +\",\" \",text)\n",
        "    text = text.lower()\n",
        "    #print(text)\n",
        "    return text  \n",
        "def preprocessing(final_string):\n",
        "      tokenizer = TweetTokenizer()\n",
        "      token_list = tokenizer.tokenize(final_string)\n",
        " \n",
        "      # Remove punctuations.\n",
        "      table = str.maketrans('', '', '\\t')\n",
        "      token_list = [word.translate(table) for word in token_list]\n",
        "      punctuations = (string.punctuation).replace(\"'\", \"\")\n",
        "      trans_table = str.maketrans('', '', punctuations)\n",
        "      stripped_words = [word.translate(trans_table) for word in token_list]\n",
        "      token_list = [str for str in stripped_words if str]\n",
        " \n",
        "      # Change to lowercase.\n",
        "      token_list =[word.lower() for word in token_list]\n",
        "      return token_list       \n",
        "for folder_name in folder_names:\n",
        " \n",
        "    # Open files.\n",
        "    print(folder_name)\n",
        "    for filename in os.listdir(os.getcwd()):\n",
        "        stuff = read_file( filename)\n",
        "        # For position and term in the tokens.\n",
        "        final_token_list = preprocessing(stuff)\n",
        "        v=len(final_token_list) \n",
        "        for pos, term in enumerate(final_token_list):\n",
        "                 #print(term)\n",
        "                 if pos <= v-2:  \n",
        "                    phrase = \"\"\n",
        "                    #print(term)\n",
        "                    s = 0\n",
        "                    if bool(final_token_list[pos].strip()):\n",
        "                         phrase += final_token_list[pos].strip()\n",
        "                         s = 1\n",
        "                    if bool(final_token_list[pos + 1].strip()) and final_token_list[pos + 1].istitle():\n",
        "                         if s == 1:\n",
        "                              phrase += \" \"\n",
        "                    phrase += final_token_list[pos+1].strip()\n",
        "                    phrase = normalize(phrase)\n",
        "                    if phrase not in bi_index.keys():\n",
        "                          bi_index[phrase] = []\n",
        "                    if filename not in bi_index[phrase]:\n",
        "                          bi_index[phrase].append(filename)\n",
        "                    bi_index[phrase].sort()\n",
        "                    \n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubo9vU0rUIbO"
      },
      "source": [
        "indexFile = open(os.getcwd()+\"/biword-index.json\", \"w\")\n",
        "pos_index = OrderedDict(sorted(bi_index.items(), key=lambda t: t[0]))\n",
        "indexFile.write(json.dumps(pos_index))\n",
        "indexFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}